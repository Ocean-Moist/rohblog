===== Current Project File Tree =====
.
├── Bring-Crypto-Back-to_Currency.md
├── building-stuff.md
├── chess_llm.md
├── early-internet.md
├── eee1.md
├── eee2.md
├── eee3.md
├── entropy.md
├── go.md
├── java-go.md
├── java.md
├── llm_learning.md
├── llms.md
├── llm_workflow.md
├── moats.md
├── my-dev-env.md
├── notes.md
├── on-my-writing.md
├── output.txt
├── out.txt
├── philosophy
│   ├── creativity.md
│   └── god.md
├── risk.md
├── startup-pipe-dream.md
├── there-is-always-someone-better-than-you.md
├── uncreative.md
├── yc.md
└── Your-Wrong-About-Hard-Work.md

2 directories, 28 files
===== Files Matching Filter: *.md =====
===== ./Bring-Crypto-Back-to_Currency.md =====
+++
title = "Bring Crypto Back to Currency" 
date = "2024-05-06" 
+++

## The Problem

Crypto becoming a commodity is the single stupidest thing to gain widespread popularity. It's the paragon of post-neoclassical/postmodern economics. If Adam Smith saw NFTs he would shoot himself. 

What I mean is that value is deconstructed to peoples own assignments to objects, abandoning the notion of any essential value in things. This is closely related to the abandonment of traditional metrics of valuations in the tech sector. 

The original idea was very good--a radical and existential threat to the existing central system of banking. Unfortunately people got distracted by money. Bitcoin was fundamentally flawed due to the built-in scarcity, so obviously the price would go up. The ideal currency has a constant, predictable 2-4% inflation to promote the velocity of money and prevent stagnation. This is antithetical to traditional crypto wisdom where crypto has been completely commodified.  

## A Proposed Solution 

Algorithmically, it is possible to control how much it costs to mine crypto at any point by modulating network fees (PoS)/block difficultly (PoW). This would in turn have an effect of the supply of the currency because if it becomes cheaper (in USD) to mine than more people will mine it (as it becomes more profitable to do so). The supply would then effect the price of the currency as on the marketplace there would be more sellers and it would still be profitable to sell for a lower price. 

Using this methodology, which is essentially equivalent to printing money, it becomes possible to modulate inflation. 

## How to determine current inflation and inflation targets 

Inflation is traditionally measured with producer price index (PPI) which is essentially the average of cost of stable (constant supply/demand) goods. 

So what would be the source of truth for PPI in this theoretical currency? 

## A Self-Hosted Marketplace

There could be a decentralized marketplace (built on top of the coins blockchain) with sellers that become trusted and vetted by buyers through some sort of rating system. 

Inflation can be measured as average price increases or decreases of this marketplace. As a bonus, USD can be listed on this marketplace and could be bought with the crypto, essentially becoming a self-hosted exchange. 

A control loop that tracks inflation could use the described method of modulating block difficulty/network fees to influence current inflation and track the inflation target. 

As always, I am open to collab and question/comments: [rganapav@purdue.edu](mailto:rganapav@purdue.edu). 

===== ./Your-Wrong-About-Hard-Work.md =====
﻿+++
title = "You're Wrong About Hard Work" 
date = "2024-01-31"
+++
## The Addictive Idea of "Grinding"
There is this idea, often sold to young men, that "you just need to choose to be successful," that success is only a function of how much suffering you can put yourself through. The reason this is addictive is that it lowers the perceived bar to peoples dream life. 

The problem is this only works if, on a fundamental level, you truly believe it. You believe that your success is unbounded. The problem is, if you consume this sort of "motivational" content, you fundamentally do not accept this as true. You are trying to convince yourself of it. 

You have to understand that the source of this belief has to be well founded. Most of the people who seek this type of story, need to reflect on what there true self image is. If you, even unconsciously, think you are a loser, no amount of consuming this hyperbolic "grindset" content will help you. 

## The Solution: Tear it all Down 
> # “Until you make the unconscious conscious, it will direct your life and you will call it fate."
> -- [Carl Jung](https://en.wikipedia.org/wiki/Carl_Jung)

To be clear, I am not Jungian. But I think [accuracy is secondary to usefulness](https://en.wikipedia.org/wiki/All_models_are_wrong). 

First thing is, reflect. Think about your actions in the third person. When you feel emotional think "I am getting angry/sad/aggressive." Every action should fall in line with who you are, try to cut out every action, everything, in your life that is not in your vision of yourself. Or, revise your vision of yourself. 

Don't get rid of emotion. Control it. Emotion is the primary language of communication between people. Feelings, intuition, predicate everything we do. How we feel about something, how we feel about ourselves, is what motivates us. This is not as insightful as you think, you know it, but it feels better to believe that our actions are "rational" (whatever that even means). 

Look within and reject any attachment to useless outside influence on your core values. 

## The Solution: Building Back Up (Escaping Troughs and Rock Bottom)
This is the hardest paradigm shift to make. Realize that people do not do crazy things out of a singular conjuring of pure will. Pulling unnecessary all nighters, that motivation you get at 3 AM, is unsustainable and you are falling behind. 

Growth is incremental, and there are infinite aphorisms I can provide to frame this: [kaizen](https://en.wikipedia.org/wiki/Kaizen) (my favorite), the tortoise vs the hare, run a marathon not a sprint, linear vs exponential growth, 1% a day for a year vs doubling in a day, etc. This is the easy part to get. 

![Linear Growth vs. Exponential Growth — Chris Danilo](https://cdn-images-1.medium.com/max/1200/1*_EslFr9qVHwSIMArMlFnRg.png)

Now the hard part: actually practicing and believing this. The single best thing I ever did was starting with taking care of myself. First thing I actually had to do was to reflect, realize that cumulatively I was unsuccessful. The first thing I noticed when I started meditating was that I was actually super tired, only awake due to the constant stimulation of the outside world. I was being controlled, my mind was malleable. I was being conditioned to be the ultimate consumer, to be an addict. I hate being unfree, so this already gave me motivation. 

There are a billion things I could say or provide, to make you take care of yourself better, but realize that the first low effort things that anyone can do that will incrementally improve themselves is self-care. 

## The Current Self-Help Culture is Harmful
Everything I just said is commonly accepted advice, and it is not wrong. However, I refuse to let this be an excuse to be lazy. If you take care of yourself and are living a linear life, in other words you stopped at taking care of yourself and your rate of growth plateaued, that is worse than the people who start at the bottom. As the people who started at the bottom accelerate to your velocity, as quickly as they caught up, they will pass you. 

We have been conditioned to comfort. Conditioned to mediocrity. Sometimes all nighters are necessary. Sometimes things must get done. Do those things. All I am saying is, think about when these are necessary (locking in right before a checkpoint in a video game), and when you are getting outpaced (the tortoise will pass you once you go to sleep).  If you like something and do not love it, change it. I am incapable of doing things halfway. I am incapable of clocking in and clocking out of work. Why would I chose to do something I do not enjoy? If I choose to do it, I enjoy it, If I enjoy it, I will pursue it relentlessly. 

This is the difference between an employee at a FAANG and a startup. It has to do with culture and mindset. If you primarily draw meaning from things outside of work, it makes sense that work is a means to an end. If you are young (no one to provide for), ambitious, and have a strong vision, do what you love. If at the first hard problem you encounter your first thought is "I am not doing this because it is not in my job descriptions and has a lot of unknowns" work at a big company, where the company cares much less about you and the value you create for it. You can easily reframe this as "this is a hard problem and how can I facilitate a solution that provides value for my company and myself?" or even "is solving this hard problem worth it in terms of contributing to the vision of the company?" Much more useful, and frankly easier to get to, framing for startups, where you are deeply connected to the vision.

## Talk Which Inspired this: [How to Win by Daniel Gross (highly recommend)](https://www.youtube.com/watch?v=LH1bewTg-P4)

===== ./building-stuff.md =====
+++
title = "Making Things People Want vs. Making Things That Alter Thinking" 
date = "2024-09-12" 
+++

I recently rewrote the interests section of my blog to be more concise. The primary interest I wrote down was **"making things that alter thinking at scale."** When I distilled what I believed to be one of my long-term goals I landed on that. 

Recently I thought about how this is both similar and different to YC's goal of "Make something people want". 

I find that successful startups do both. People want it and it changes the way people think. Here are a few examples. 

- For Uber, people wanted an easier way to get around, but, at the same time, people's thinking shifted away from "why would I want to get into a strangers car?" 
- AirBnb shifted thinking away from "why would I let strangers into my house?". 
- Reddit changed the way people interacted online and how online content was aggregated, people often append "reddit" to their google results to get higher quality content. 
- DoorDash changed the way people think about takeout and how restaurants monetize their food.
- OpenAI completely shifted how people think about intelligence and the effects of AI on society. 

I think altering thinking at scale requires people wanting that thing, otherwise you wouldn't get network effects and scaling would be impossible. 

Also, I think that making something people wants requires altering the way they think. When you make something new you ask people to reconsider their existing habits and first principles. This mental shift, however small, is what leads to mass adoption and users falling in love with your product. 

Since they both require eachother, in a more abstract sense, we can say they are equivalent. Treating them as equivalent leads to interesting outcomes. 

The more something changes the way they think, the more they want it. Think about how Google changed the way we think about learning and accessing new information.

Also, then, asking people "does x change the way you think?" is sometimes a more valuable and better phrased question than "do you want x?". I think this shift in perspective could also help people escape tarpit ideas, as humans have a hard time figuring out what they actually want.     

Because these two are related, I think the best piece of advice from this essay is that **if you are trying to get people to want whatever you made, try and get whatever you made to change the way they think.** 

tl;dr: making things people want and making things that alter thinking  are isomorphic to each other 
  

===== ./early-internet.md =====
+++
title = "I Wish I Didn't Miss the '90s-00s Internet" 
date = "2024-09-06" 
+++

## about me
I am 18, born in 2006. This is generally a good thing as I am in the prime of life currently. I am not one of those people who think they were "born in the wrong decade", I think I was born at the perfect time to take advantage of superlinearly growing technological advancements.  

## the internet today
I generally greatly dislike social media, although I am an avid user of it. When social media caught on, the people running these companies were tasked to make it profitable. In doing this, social media got completely ruined. 

Our data got commodified[1], our attention got commodified, and a substantive part of who we say we are got commodified. This, in general, has led to a degradation in the quality of the internet.  

They basically made social media like a drug, as addictive as possible. They do this by promoting FOMO and comparison in Instagram's case. Instagram is a game, it is extremely performative. People carefully curate each part of their insta to give certain impressions. 

What's the ratio of followers to following you have? Are your story highlights organized and "aesthetic"? What reels are you liking? 

There are a lot of "rules" in this game, which are enforced by social "ins" and mutual respect. 

When it comes to shortform content, hundreds of people compete for slivers of our attention. We are not agents in this, they are just presented to us. Completely depersonalized. They are forgotten within seconds. Ask someone watching tiktok to describe the previous tiktoks they just watched, they would be hard pressed to tell you more than a few minutes in the past. Something about tiktok is unusually addictive. all the while providing absolutely no value.  

It has become so shallow, you can tell almost nothing about who someone actually is through Instagram or tiktok. You can only tell how they want to portray themselves to the general population and, by how they organize their profile, if they are eligible to be a part of your social circle.  

## the appeal of simplicity 
I wish I was around when people had blogs or even myspace. This era was deeply personal and creative. Most writing on the internet was individual, not written in search of "SEO" or profit but driven by the need and want of people to share knowledge--pure curiosity. 

I want the thrill of finding new websites searching through web rings; when the web was truly the wild west and not another arm of control by mega corporations.   

This is also reflective in the quality of content. There was little incentive to lie, to manipulate truth, and each blog entry or piece of information was tied to identity. (except in the cases of anonymity). 

Even the content written by normal people for normal people has been commodified by sites like reddit and quora. What happened to an old fashioned forums or even usenet groups? (granted, especially for cars and hacking, there still exists plenty of forums)

Also, websites just simply looked cooler. Occasionally I scroll on the geocities archive and wonder, how did we get here?  What happened to the patterned backgrounds, the bright maximalist jpegs and gifs?  This is sort of contradictory to my website, as it's almost annoyingly minimalist, but this more has to do with social norms and simplicity. Having a personal blog is already out of the ordinary, but the simple design and clear technical direction/theme gives me an excuse. I also am not that personal on here, because only a few of my friends frequent my blogs, and I want the site to be as simple and to the point as possible if a random person wants to know who I am. 

## a niche resurgence 
There is [neocites](https://neocities.org/), and a small community of people who share this philosophy about the web (and that are relatively young), but I have not met anyone my age, in the real world, that would choose to do something like this. 

The majority of people (my age) today would think sites like those (and, by extension, their creators) are weird.   

[1] you can substitute "commodified" with "bought and sold"

===== ./java.md =====
+++
title = "Java is an amazing language (better than rust)" 
date = "2024-03-05" 
+++

DISCLAIMER: Let me preface this by saying my favorite language is Haskell and that I write toy languages for fun. I am a pl nerd so my opinion is valid. Also the title is like 20% bait. 

My secret was always that I like coding in java, in some sort of masochistic way. Everything is dead simple. Everything is so verbose you can never forget what anything is. It is like sitting down and playing a relaxing game of stardew valley. It flows so easily. The feeling of mastery is so easy to attain. 

Coding in rust is like playing chess and league at the same time. It is the same mental exertion and the same insufferable people yelling at you on the internet. The semantics are cool. I love how it borrows stuff from functional languages. But can I tell you a secret? Java gets 75% of the way there with 10% of the complexity. 

Error handling is definitely better in Rust, but it is closer than most think. The way errors get buried and propagated up the call stack can be confusing. In complex programs it can be missed. Pattern matching is better in Rust, but is closer than most think. Java records, java switch statements, Optional<>, streams, they all basically make up for the OOPiness of the language and are all pretty straight forward. Also the concurrent data structures in java are much nicer to use than in rust. 

Learning rust is hard, it could not be the language AP CS is taught in. The first skill gap is the borrow checker, then lifetimes, then async, and then macros. The smallest, first skill gap, the borrow checker, was about 10 times harder than learning about Java classes for the first time, and about 3.5 times harder than grokking monads. I still can  BARELY write complex async rust, and if I see or have to write a macro that is like a hair above the simplest examples I will quit the editor. 

I would say the most clusterfucked abstract class inheritance system design in Java would not even approach the most complex Rust system design. Then again, I have not worked in a legacy Java environment, but I think this argument is still valid. Rust simply does not scale as well as Java. 

I also hate the skill issue argument. Like you know what the ultimate skill issue is? You not knowing C well enough to make memory safe code. Forget C, you not going complete rollercoaster tycoon and writing some plain x86 assembly. 

The whole point of a language is to be easy. Basically the Go philosophy (which I do like more than Java these days, although it does not have the functional stuff I want). Like the whole point of high level languages is to never encounter such skill issues. It is to write working code fast and in a maintainable way. Naturally you would want this to be easy. 

It is so easy to write good java with a few guidelines. Duplicate 1-2x before abstraction. No inheritance unless you are sure, especially no more than one layer deep. Use Optional<> over null. Do not get cute with your abstractions. Java is so easy people feel the need to make it hard doing OOP gymnastics to build "elegant" systems. 

Now here are some cheap, easy arguments. More people know Java and more jobs need Java. Tooling is better for Java and the ecosystem is more developed. I absolutely love JetBrains IDEA and no amount of loser lua neovim (don't come after me I use neovim for C, zig, and other low level languages) config for Rust will match IDEA's support for Java. (also if the primeagen is reading this, lua sucks, don't @ me, it is never enjoyable to write, imagine liking the language I used to code roblox hacks in when I was 9 years old).

I am going to admit, maybe I just do not know rust enough. But I promise I have coded more rust than most of you have, and I probably know it better than the average self proclaimed rust enthusiast. And yet, if I had to write a big grug "business logic" project, you would see me reaching for Java before rust.  

The final thing about rust is that it is not even particularly elegant. Haskell is nice to look at, APL (BQN, UIUA) is nice to look at. When I want a puzzle I write in those languages. Because I admire the mathematical abstraction. Rust is marred with the compiler, marred with low level BS that makes it ugly. That does not make it satisfying in this way. If you love language design, or beauty in programming languages, Rust is not even that good. It is not beautiful, it is not as useful as people proclaim, and it occupies such a small use case that the language is mid.  

Even when I do systems programming, like low level stuff, I just want to manage my own memory. I do not want to beg the compiler to let me write code, I just want to allocate stuff and put stuff into there. C is fine for this. I'll try zig later. 

And I am not saying rust is always bad. It has a small but limited number of use cases. For example I wrote a bot that helped buy things rather quickly in 2020-2021 (provided I was awake to do some captchas). I needed it to be performant, robust, and the scope was just simple enough (even though it was async) where I knew I would not need a lot of code (but a mediocre amount). It occupied the space just before the complexity curve went asymptotic. The other use case is for writing toy languages, it does that super well. At its best it feels like using a cnc mill to machine a precise gear. But most times you just need an injection mold and some plastic. 

P.S. check out this fizzbuzz impl. I did in Java, I have not found a similar solution in Java ANYWHERE on the internet. It also shows the power of modern Java. (please do not actually use this in an interview) https://github.com/Ocean-Moist/FizzBuzz/blob/master/src/Main.java  

P.S. plz write your applications in Go instead of Java, otherwise you might get brainwashed by RxJava. if this happens, no one has found a cure. 



===== ./llm_workflow.md =====
+++
title = "85% of Cursor AI in a Shell Script and Good Prompting" 
date = "2024-10-22" 
+++
This is an example workflow of how to integrate LLMs into your software dev.  

First script (`prompt.sh`) does one thing: dumps your entire codebase context into a text file. It first appends the file tree while ignoring what you tell it to ignore (node_modules, files not needed contextually, etc), and recursively finds and auto includes what you want to include (e.g. *.ts, *.tsx), and adds any extra files you need (API docs, project plans).

Second (optional) script (`create_file.sh`) takes the LLM output and creates files in the right path. This is based of the fact that you prompted the code generation LLM to prepend files with something like this:
```js
// src/app/project/components/ProjectCompletionScreen.tsx
``` 

Both scripts are available on my github and linked at the end. 

The workflow:
1. Write (use o1-preview to write) a `project_plan.md`[1]. Write any other docs you want it to reference, I wrote a UX design guide with the colors, fonts, and style I wanted to use. Write what you think a junior dev would need to complete the project on their own. 
2. Set up three prompt templates/files:
   - task_giver_prompt: Asks the LLM what to do next (but doesn't write any code, just makes a plan, like what component/file to next create/implement)
   - llm_task_proposal: Where you paste the output from the last guy
   - task_execution:_prompt The prompt for the LLM that does the actual implementation  
3. Run it:
```bash
prompt.sh -i"node_modules" -e"*.ts *.tsx" -f"plan.md,task_giver_prompt"
# This command will make an out.txt with your entire project and all the docs/prompts specified
# Copy out.txt to o1-mini (using xcopy for convience)
# Copy o1's response to llm_task_proposal_prompt

prompt.sh -i"node_modules" -e"*.ts *.tsx" -f"plan.md,task_execution_prompt,llm_task_proposal"
# Copy output to Claude 3.5 Sonnet (using xcopy for convience)
# This time we want to include a different prompt and the output from o1-mini
# Use create_file.sh to make the file for the response quickly. You run create_file.sh, paste the code, and press ctrl_d. It will make the file in the right directory, instead of manually having to create the file and then copy/paste-ing.
```
4. Review the code. Fix any errors. Start a new chat. Repeat/tweak 2-3 until project is done.

You get the improved recall and reasoning of o1 with the code gen ability of claude. If your project gets too big, decouple your code and write better docs to lower context, and lower the scope of your project plan.

You/others can work on it and leave stuff unfinished and the AI will pick it right back up. It is not on the level of doing stuff on its own yet, so you need to still write code and be in charge of it. 

The magic is in crafting the prompts, so look at some of these examples:

**Example Prompts**
-   **Task Giver Prompt**  
    [_View on Github_](https://gist.github.com/Ocean-Moist/87cb55ef5d7664802326f79a400c570d)
    
-   **Task Execution Prompt**  
    [_View on Github_](https://gist.github.com/Ocean-Moist/a0721110b72ad5aee99bab7a10757ff1)
    
-   **Project Plan (`plan.md`)**  
    [_View on Github_](https://gist.github.com/Ocean-Moist/57c8b3d1bc8d6003f7c81c16eda55284)

Scripts as promised:

**Scripts**
-   **prompt.sh**  
    [_View on Github_](https://gist.github.com/Ocean-Moist/dec0fc92723fe9bcfcefc041d233fab6)
    
-   **create_file.sh**  
    [_View on Github_](https://gist.github.com/Ocean-Moist/68ce4ea771314d7d96aba0ed12c68cea)

## notes
[1] Keep project plans clear and scoped. Take the guesswork away from the LLMs. Let them focus on one component at a time.

## P.S.
This is an example. Scripts need work. prompt.sh args are messy and should auto call xcopy. create_file.sh should handle multiple files. The magic is in the prompting--tweak until it flows. I am in the process of making something 10x better email me if interested **rganapav [at] purdue [dot] edu**. 


===== ./llms.md =====
+++
title = "Writing Code with LLMs Leads to Better Software (and other insights from building LLM systems)" 
date = "2024-10-08" 
+++

Recently I have been working on what I guess is called an LLM agent, but it's more just building more ergonomic tools for myself to use LLMs in my life. The code base keeps growing, and I use it to help write itself.    

What I have noticed is that the patterns I have applied for making code more intelligible to LLMs also apply to making code more intelligible for humans. I think the reason for this is that LLMs can only successfully hold so many details in their context window, and we can only hold so many details in our mind. These observations may seem obvious, but what's so interesting is that two different sets of constraints lead to similar invariants. 

Generally if you `find . -name '*.go' -exec cat {} + >> out.txt` and just append that to the prompt, the LLM will get confused if it's being asked to do something mildly complicated and if out.txt is over >5k lines (degraded performance around 2-3k). This is like if we had to keep a whole API, implementation details and all, in our head while using it. 

When faced with this issue there are a few easy optimizations. The Cursor AI method is to selectively provide a subset of relevant files. This works up to a point, because at some point, there are way too many relevant files and definitions. In order to decrease the amount of relevant files, you need to decouple your code and modularize it, so parts can be worked on non-monolithically, which is also general coding advice. 

Further, extracting common actions to an API, and then writing api docs so that the user (or LLM) does not have to concern itself with implementation details is another way to reduce the necessary context size. Initially I just wrote (or, rather, the LLM wrote) API docs for the backend REST api so the LLM could work on the frontend without concerning itself with backend code, but then my backend got way too complex. Then, I wrote API docs for my external API wrappers (I had go wrappers around OAI, Claude, GPTZero, et al) and other functions.

Overtime I split my project up using these es so that I could use LLMs more efficiently on them, then I realized I inadvertently made it easier for *anyone* to work on my project. As someone who codes a lot as a single person (and not as team) and who hates premature optimization,  I start (and end) most projects as a monolith. When you are the only working on a project you develop seemingly obvious intuitions just by working in the problem domain for a long time. I generally also don't write docs for solo projects except for a TODO list and spend little time worrying about naming.

So writing code for/with LLMs makes you write better software. I think this is a very counter-intuitive point. 

 ## general tip for coding complex projects with LLMs or making LLM agents

I find that LLMs have terrible intuition for creating big project's architecture, especially 0-shot. I am trying to solve this problem via prompting and other stuff in the very tool I am building. Even o1 models (though significantly better) still suffer from this issue. 

LLMs are maximally advanced beginner in all fields and will just make things that don't make sense and have terrible intuition. You basically have to supply intuition. There are 3 steps to software development: describing goals/functionality (planning) -> architecting -> implementations. The one LLMs struggle with is architecting. If you ask it to create an API that does XYZ on a high level it will struggle. If you ask to clarify what XYZ is it will excel. If you ask it to implement a, b, c API routes via implementing d, e, f functions it does pretty well. 

If you've been working in a large, monolithic, project for a long time, and you are tasked with implementing a simple feature you will think it's easy. You'll just define the shapes of the functions and fill them in, aligning with the patterns of problem solving used in the program. If you dump 5k LOC into an LLM and tell it to implement the same feature it will do terribly and the functions and their shapes will be convoluted or not clearly help solve the business objective.

To be fair, if you dump a new person in the same code base they will probably struggle too. The average new hire probably (hopefully?) is equivalent to o1-mini in coding. 

## random stuff/advice
Create new chats/contexts as often as possible and optimize for only including necessary information and compressing that information into the smallest amount of intelligible tokens as possible. 

I also noticed o1 tends to fix badly/complexly worded prompts so prompting is less important with it, you just have to prompt for guarding it from doing unrelated/unnecessary stuff.  

I find that after the initial gains I am somewhere between 1.5-2.5x more efficient at coding when using LLMs. Cursor AI value add vs. basic bash tools + LLMs is maybe around 20-30%, not worth leaving Jetbrains.  

ChatGPTClaude and other chat based interfaces have terrible UX. Cursor is better, but value prop is too weak for most actual programmers to switch. I'm thinking m going to neatly package all the tools I use, call it rhoGPT, and launch it. I suggest you write your own tools/agents, as interfaces with LLMs is (what I think) the next billion dollar problem.  

P.S. `find . -name '*.go' -exec cat {} + >> out.txt`, knowing other terminal text processing tools (sed, awk, etc), and vim/editor keybinds is the best 80/20 choice to implement creating prompts fo LLMs for coding. I suggest writing simple tools for creating prompts in bash first, you don't need anything complicated. 

I have a folder of common prompts in plain text files then I mix and match them using the bash tools. 

Perchance I am at the rare intersection of maximalist gen z LLM users and suckless driven linux hackers, but this workflow is surprisingly productive. 

Ask me question/provide feedback/share what interesting thing your building at my email--I reply to >95% of them. :) 

rganapav [at] purdue [dot] edu OR rohanganapa [at] gmail [dot] com

===== ./my-dev-env.md =====
+++
title = "My Dev Enviroment" 
date = "2024-04-05" 
+++

### The Laptop

Before I broke it, I used a Zephyrus G14, it was great, the best mobile AMD cpu, and a pretty good gpu. 

It was probably the best laptop on the market. Great cpu for compiling stuff (gentoo moment), and great gpu for VM passthrough so I can send texts and write swift. (sorry apple).  

I am now using a thinkpad x13 yoga, the folding and touch screen is gimmicky and I don't use it. I will always prefer taking notes and reading on paper.

### The OS

Linux is a must. Linux supports the freedom of the user. I mean everything can be built to be the most efficient and best for your needs. I would use BSD (specifically, openBSD) if it was not for lack of support

### The Distro

I used to be a chronic distro hopper until gentoo. Gentoo is the single greatest distro to ever exist. It is one of the last distros that supports user choice and it shows. 

I turn bluetooth off on the kernel level, but otherwise keep it generic. I use OpenRC because I hate systemd. That could be a whole article in itself. I will give one brief example. I want to change my DNS on ubuntu, I open up resolv.conf and what do I see? ITS MANAGED BY SYSTEMD. WHY IS MY INIT SYSTEM STICKING ITS SLIMY TENDRILS IN EVERYTHING I DO.

### WM and userspace

I use DWM and slstatus from suckless (with my own selection of patches of course) and dmenu. I use Xorg because I am used to it and it just works. I tried wayland a few years ago and it was not stable. I use ly for the display manager. I however use alacritty and not st because it is super fast and I like the config. I use networkmanager. I use chrome for browsing because I am trapped by the sync. If you are looking for browser recs, I would just recommend ungoogled-chromium. I use a local password manager. 

### Dev stuff 

I use neovim for basic file editing and config stuff, but I absolutely love jetbrains IDE products. I am sorry I do not want to spend hours of my time to reach feature parity with an IDE that takes negative time to setup. Actually, I think the quick fixes in IDEA (java) are way better than any neovim plugin. Though, I of course use vim keybindings, there is no better way to edit text. 

### Jetbrains Stuff

I used to use copilot, but its actually not helping that much.  If I need AI to do boilerplate or generic stuff I just paste into claude and it does an ok job. Obviously I use ideavim. I use rainbow brackets and rainbow indents and rainbow csv. I use nyan progress bar because it might have been the first plugin I installed. I use this weird theme called soft-charcoal, but most people would not like it. 

### Terminal stuff

I am back to bash (use to be dash and yash). I am tired of rewriting every little script to remove bashisms and make it posix. I used to use nmtui but now I use nm-applet. I use the GNU utilities, I am not a shell wizard but a few tips I can give are as follows. Learn GNU parallel. Learn grep, sed, sort, etc. Will get you out of a pinch. I am not a terminal wizard. I used to use nushell but it was too complicated and I never need to use the terminal in that way. If bash gets too complex I will just reach for python. 

### Conclusion 
I started using linux daily about 7-8 years ago, when my time then was worth nothing and my brain was not developed fully (it still is not close, to be fair). I chose to make the investment into my build tools so when my brain developed some more it would payout. In other words, I made this investment when my time was worth nothing (although it is only worth a bit more now) so it would pay out when I started building stuff. If you have mac and work in industry, or are a junior or senior in college, I do not see Linux being a worthy investment unless you have a genuine interest in it. 

Funnily enough, 7-8 years ago, in the same vain I tried to learn DVORAK and vim motions. I will let you guess which one stuck. Never use DVORAK. I would also recommend just doing what interests you. At the time Linux did (it still does, again, to be fair) so I used it.  

Feel free to ask me about more stuff and I will add it here. 


===== ./notes.md =====
+++
title = "My Stationary Stack" 
date = "2024-09-11" 
+++

## inspiration
I frequently lost pencils and never took notes for most of my academic career. Eventually I got to the point in my self-studying and in my learning were notes was necessary. I find that writing things down can 2-3x the amount of stuff I can hold in my head at once, while making it easier to reference, and makes me remember it for longer and quicker.

## pencils
I prefer pencils to pens as I can erase pencil but not pen. I also enjoy the experience of using pencils more. 

I have an arrangement of mechanical pencils with several different purposes. I have a rotring 600, a pentel orenz nero, a graphgear 1000,  a  zebra M-701, and a kuru toga metal advance. I use pentel ain stein lead in 2b, I find this darkness is a good middle ground for writing. 

The rotring 600 and pentel orenz nero are both in 0.5 mm, perfect for every day writing. If I am doing math or writing to be shared I generally use the rotring 600 as I find it has a hard and precise nature. If I am going to write large amount I use the pentel orenz nero as I find it soft and gestural. These two are my favorites. 

The graphgear 100 is in 0.3 mm, I use this for fine lines and graphs when I do math. The zebra M-701 is in 0.7 mm, I use this for thicker lines and this is the pencil I give to people if they ask for one (as it's the cheapest and pretty heavy). 

The kuru toga metal advanced I also use for writing large amounts of text, I find it to be sharp and light. I use this primarily when I am tired or bored. I mainly just got it because of the rotating pencil led gimmick. 

I also have a few blackwing palominos (the new ones). They are decent and I sketch with it. I also have a set of koh-i-noor progresso woodless graphite pencils and a set of faber castell polychromos colored pencils I use for art.  

## other tools
I have two rulers, a really cool nvidia one I got at nvidia GTC, and a classic red digi-key ruler I got when I ordered something once. Both serve a similar purpose. For drawing I would recommend getting one that has a corked back so it's less likely to move.

In total, I have 4 calculators:
1. I have a numwork n0110, which is the best and most intuitive calculator I have ever used. Custom firmware is possible and (in the factory configuration) it is allowed on the SAT and AP tests. 
2. I have a TI-84 Plus CE Python, because some of my teachers are (rightfully) suspicious of my numworks. I also let people borrow this one, and use this if my numworks dies.
3. I have a Casio fx-991ES which is my backup scientific calculator. This calculator I also let people borrow. 
4. I have a classic TI-30X IIS in pink. I use this as my main scientific calculator. The pink is a nice touch and it stands out so I don't lose it among the classroom calculators. This is also the calculator I used to take the SAT and has unrelated personal memories. If I am feeling nice I let people borrow this one as my casio is somewhat difficult to figure out how to use. 

I use a staedtler mars plastic eraser and a kneaded eraser I mainly use for drawing. 

I use a blank 8 1/4" x 12 1/2" rhodia notepad. These are stapled like legal pads but white and blank. I use to use lined legal pads, but the lines and yellow color, along with subpar writing surface turned me to these rhodia notepads. 

I like the extra length given by the notepad, and I find if I get dotted or lined pads, I tend to just ignore the lines or dots. 

## why?
I find that by taking pride in my tools, I tend to respect and enjoy using them more. I have not lost a single one of these items (except for one time where I lent my numworks to someone and they lost it...). 

I also understand most of these things are expensive (relatively, anyways), but budget alternatives do exist and I recommend people to do their own research as it makes the process more personal. 





===== ./on-my-writing.md =====
+++
title = "On My Writing" 
date = "2024-09-05" 
+++

In 9th grade my English teacher told me I was writing an assignment with broad over-generalizing maxims and beliefs. I tend to write absolutely, especially when being un-technical/un-scientific. I think this is because this makes my writing more concise and more clearly communicative--unencumbered by the recognition of my limitations or the limitations of my ideas. 

This is because I take for that granted everything is limited. No idea or set of ideas can be truly be always "true". Truth is assigned and constructed anyways, so most of my writing gives simplistic systems ("heuristics") that are useful for making sense of the world.

My english teacher also told me that I lacked ethos. I still lack ethos. I would not take advice from me either, there is little proof that my ideas lead to success. This means if you are reading this you are wasting your time (unless it is the future and I have managed to make something of my life). The content you consume effects your ideas, and generally this heuristic of taking advice from people who have accomplished what your goal is is a great way of filtering noise. 

I have some personal conviction, however, so I actually do believe these ideas. The problem with listening solely to successful people is that everyone else is doing this, so you are average. You'll be "right" a lot, but it won't be as valuable as people who are simultaneously "right" and contradicting the majority narrative. So the self has some right to be a north star. 

I almost never took notes in school, instead relying on becoming familiar in broad strokes what is being taught and filling in the rest with reason and logic. I do the same thing with most of my thinking, but sometimes I find myself doing redundant work. 

When I write, I write the filling and not really the broad ideas, so I can quickly "rebuild" this knowledge if I lose it. My ideas are somewhat spiky in their generation, nonlinear in their creation, so sometimes I will have a bunch of great ideas, and if I don't write them down the next day I will just be left with the wisps of them, and sometimes I can't fill in exactly what I thought about. 

I think, as a general rule, all knowledge should be free, useful or not, so that's why I cast my writing into the internet. Maybe 10s of people will generally find my ideas interesting, which is enough. 

If I were to give myself advice 4-5 years ago, even 2-3, I would tell myself to document everything I was doing. 

===== ./philosophy/god.md =====
+++
title = "Science Didn't Kill God" 
date = "2024-08-12" 
+++

## preface 
This article is agnostic to if you believe or don't believe in God. It is about the dogmatization of science.  

## the problem
Unfortunately it is becoming common to portray people who are religious as dumb and claim that God is dead due to science. I don't think this argument is fair.

Our mathematically understanding of reality (physics) is not reality, We are not uncovering the hidden code of the world. It's all symbolic, operations over symbols that aid in survival. 

We impose our mathematical models on reality in an attempt to make predictions. These mathematical models are structures we have made up to cope with the fact that the universe is a black box. Physics experiments are second order testings of what is actually happening.  They are second order because our perceptions are disconnected from reality (if there is one). They don't test what is really there. 

It is like a deer who sees some movement and classifies it as a predator. It has evolved to do that. It cannot distinguish that some "predators" are non-threats. We are like the deer. I did a bad job of explaining this. See [this video](https://www.youtube.com/watch?v=oYp5XuGYqqY). 

So we invent these systems because they seem to somewhat match what we observe and measure. And this is useful, no doubt. But it is not some secret of the universe. As an example, the popular equation e=mc^2 is not true for all scenarios. A more accurate equation is e=γmc^2 because of special relativity. This sentiment is put into words by the popular saying "all models are wrong, but some are usefull". We only say thse equations are only right because we did some weird stats and got some p-value confirming they work under limited preconditions under limited precision. 

We do not know for certain that any equation is always true. In fact, truthiness is an invented concept, loosely defined in relationship to surviving. There is always some p. And we bet our lives on this p, everytime we start our cars or walk on a bridge. Science is very useful for existing, or for living rather. But do not mistake science as the secret to the universe, or give it some godlike certainty. 

This concept of uncertainty is baked into what science is. It is a process that produces an agreed upon result, this result is not assumed to be correct and is poised to be challenged at every step of the way. If you declare science a god replacement, you commit a cardinal sin of science and declare the findings what science is about, when it is really the process. 

So science has, in fact, not infringed upon God, or subsumed the need for a God. You don't "believe in science instead of God," because these things are incomparable. Don't minimize people, or God conceptually, by using science. 





===== ./philosophy/creativity.md =====
+++
title = "The Entropic State as Creative Necessity"
date = "2024-10-31"
+++

## the analogical physics of creativity

Consider paint mixing: two colors swirling together create temporary, unrepeatable patterns. These patterns exist only in transition—between separation and uniformity, in a parallel to mental states. Like those paint swirls, certain cognitive configurations exist only in transition, in states of productive instability.

This state mirrors the thermodynamic concept of entropy: a measure of a system's possible microscopic arrangements. High entropy means many possible states; low entropy means few. But the interesting phenomena—life, consciousness, creativity—occur in regions of medium entropy, where there's enough order to maintain structure but enough chaos to enable change.

## deconstructing common framings of binary states 

The traditional framing posits a false dichotomy: order versus chaos, peace versus discord. This not only surpasses nuance, but necessarily limits the interim as not living up to either edge. They aren't compromises between extremes but their own distinct configurations, operating according to their own logic.

Consider how we process information: not through pure order (which would be mere repetition) or pure chaos (which would be noise), but through patterns that emerge from controlled instability. Our neural networks operate at a critical point between order and randomness—what physicists call the "edge of chaos."

## the necessity of instability

Peace, in the Buddhist sense of complete mental quietude, represents a state of minimum entropy. Complete chaos represents maximum entropy (a total destruction of cause/effect, of any semblance of connection to free will). But creativity requires maintaining a state of medium entropy—a zone where novel patterns can emerge and persist long enough to be captured.

This isn't about finding "balance" but about inhabiting a fundamentally unstable state. Like a bicycle, which is stable only in motion, creative thinking requires constant movement, constant processing of discord into novel configurations.

## the productive export of chaos

Local decreases in entropy—the emergence of order, of ideas, of patterns—are possible only through the export of entropy to the broader system. This is a fundamental principle of thermodynamics, and it applies equally to mental processes.

When we create, we're not eliminating chaos but channeling it. The apparent order of finished work masks the necessary disorder of its creation. Every clear thought emerges from a background of productive confusion, every elegant solution from a tangle of failed attempts.

Broadly, human experience is characterized by transience. Change is all that is known, much of the philosophical (and human) project is to develop systems that seem to provide “stability.” We can point to religion and science as these so-called constructed sources of stability. Religion seemingly being undressed by a reliance on “faith” and science being undressed by the necessary uncertainty in results (instability among scientific results is sanctified by the system/process, and formalized through statistics).[1] 

Awareness of this broader challenge, of this systemic source of progress, enables more productive results.

## beyond progress narratives

The conventional narrative suggests creative work progresses toward completion, toward resolution. This misunderstands the nature of creative states. They don't progress toward stability but maintain productive instability.

Consider mathematical thought: it advances not through steady accumulation of certainties but through productive confusion, through the deliberate destabilization of established understanding. Every major breakthrough has required mathematicians to inhabit states of profound uncertainty, to dwell in the contradictions that signal the edges of current understanding.

## multiplicity of states

Our mental states exhibit multiplicities—simultaneous, overlapping configurations that resist reduction to single descriptions. Like quantum systems before measurement, creative thoughts exist in superpositions of potential states.

This multiplicity isn't something to be attacked with systems of habits and stifled by perceived intellect. It enables the novel connections, the unexpected insights that characterize creative work. The attempt to eliminate this multiplicity, to achieve perfect clarity or peace, would eliminate the very conditions that make creativity possible.

## practical implications

This understanding suggests different approaches to creative work. Instead of seeking to eliminate uncertainty or anxiety, we might learn to inhabit it productively. Instead of pursuing clarity at all costs, we might maintain productive ambiguity long enough for novel patterns to emerge.

The goal isn't to achieve tranquility but to maintain the right degree of instability—enough to enable new configurations while maintaining coherent structure. Like those paint swirls, our most productive mental states might be inherently temporary, existing only in transition.

## embracing productive disorder

The world benefits most from minds that can maintain states of productive instability—not because they've achieved perfect peace or succumbed to chaos, but because they've learned to inhabit the space between, where entropy hasn't yet had its final say.

This isn't a celebration of suffering or a rejection of peace. Rather, it's a recognition that creative (progressive) work requires us to maintain states that are, by their nature, unstable and temporary. The art lies not in achieving stability but in learning to work productively with instability, to capture the patterns that emerge in transition before they dissolve into uniformity.

I find that my existence is defined by tension between peace and anxiety, on the cusp of failure and knocking on the door of success, between an outward perceived superfluous ego and inner anatman. This perspective is an attempt to communicate a productive system of working under my own assumptions about my own experience. 

[1] This paragraph on religion, philosophy, and science is perhaps too dismissive of human progress and does not provide a complete critique. One must note that it is impossible to construct a fully valid interpretation or system of understanding and that faith underpins all of any sect of the human project.  


===== ./risk.md =====
+++
title = "Risk" 
date = "2024-08-24" 
+++
Most people fundamentally misunderstand risk. 

Risk is a broad, somewhat useful, metric that people use to predict future outcomes. Risk is only correct relationally to the system that its defined by. 

It's a way to quantify a lack of knowledge in relation to predicted future outcomes. Risk is inherent to all systems designed to predict the future as all systems of human understanding are limited. 

So how to succeed in high risk circumstances? Know something others don't, or better, know something that others think is false. These asymmetries in knowledge lead to arbitrage that can be exploited. This is, atleast, how people will explain highly atypical outcomes after the fact. 

It's more useful to ignore risk. Risk is noise, rather a reaction to noise. A flawed prediction based on flawed knowledge. Risk is meaningless. Just build, no one can come close to modeling the extent of human creativity. 

You don't think about the risk you take driving to work, you just drive. It's not useful to do something *and* simultaneously think about risk. 

A lot of things are like this--entropy or fuzzy probability used to attempt to quantify logical unknowns. This should at least tip you off to the legitimacy of the system. 

===== ./startup-pipe-dream.md =====
+++
title = "Is the College Student Startup Pipe Dream Dead?" 
date = "2024-09-13" 
+++
## The college startup pipe dream
I write this article to pose a question: Will the next trillion dollar startup be started by a college-aged kid in his dorm? 

Can the pattern of Microsoft, Meta, Amazon, Apple, and Google be repeated?  (Bezos being a sort-of exception to this, as he was ancient when he started Amazon at 30). 

## An analysis of historical successes 
What's interesting is that all these incumbents started as consumer startups, and most hyperscalable startups that have started in the interim are B2B SaaS. Looking at the exceptions, even the newest fastest growing social media incumbent (TikTok) was started by a company not an individual. Past that, the next biggest consumer innovation was ChatGPT, and that was created by a team of research scientists, a far cry away from a guy and his friend in a dorm. 

Then there are the Uber's, the Doordash's, the AirBnb's, et al. But these companies are quite limited in their scope, and clearly nowhere near trillion dollar scale. It doesn't seem like they have a way to get there either. I don't say this to downplay their extremely impressive achievements, just in pursuit of an accurate understanding of the environment. Further, these founders seemed to be in their late 20s/early 30s when starting their business. Even the hard data suggests that most unicorn founders are in there mid 30s.[1] 

I think there are a few reasons for this. The first being that these big companies have become effective monopolies on scales never seen by capitalism before.[2] The second being a relative stagnation in the technological platforms being used. People are sort of set on phones, laptops, and the web. They have been out so long that almost everything within the bounds of cognition has been tried. 

Apple and Microsoft took advantage of the rise in personal computers (and then Apple, again, took advantage of the rise of smartphones). Amazon, Meta, and Google, all were built off the rise of the web, and then had a resurgence with smartphones. On the backend, there is the switch to cloud and horizontal scalability, which a number of these companies also took advantage of.

The ways people interact with technology have only gotten sub-linearly better since then. We have been treading in the bottom of the log-loss function for so long, all the easy gains have been optimized away. 

## Looking towards the future (AI)
The obvious next question is about AI. Do the improvements in AI constitute a major platform change? If so, then the college student startup pipe dream has a glimmer of hope. 

I generally think so. This is for a few reasons. 

The first reason is that this technology is not really an incremental improvement over the previous RoBERTa type models, and I think few people understand how surprising it is that these scaled markov chains approximate intelligence. 

The second reason being that ChatGPT and other LLM based tools I have hacked on/played with are much better to use and interact with than their alternatives. For example, one of my friends is building an AI-first CRM for Real Estate agents, and customers seem to respond well to it. Further, my intuition tells me there are a lot of unknown unknowns when it comes to the application of this technology. 

The last reason is that I am sort of blithely optimistic that technology can be growing on a Moore's law-esque scale for a little bit longer and that someone like me (a random college student) can actually change the world on the largest scale. Call me foolish.  

I think looking at OpenAI and Anthropic, you can be fooled into thinking that no one can compete with them, I think you would be wrong. 

I generally think ChatGPT as a product is of limited use to anyone who is not a programmer or student. Maybe writers can game some feedback and overcome writer's block, but that use case also seems extremely limited. Even in those use cases, the UX is kind of just bad. 

There are a lot of ways to package and build around LLMs. I think a clever college student will put some puzzle pieces together in such a compelling way that it will change the way we think. I can't really imagine what that would look like (otherwise I would just build it), but this is what my intuition tells. 

Apart from raw text and on the model side, it's also surprising how model performance scales with data. I think pretrained transformers can be applied to areas other than raw text, and that encoding and decoding higher order relationships is extremely powerful tool. If you are a curious person, I would highly recommend checking out [nanoGPT](https://github.com/karpathy/nanoGPT) and learning about making your own transformers. Do not think that only OpenAI or Anthropic can produce model level innovations. There is a large market for "foundation models" as the VCs call them.   

## Postscript 
Credit to my extremely smart dad who's conversation inspired this article. He was trying to get me to focus *a little more* on college instead of writing useless blog posts or playing with the shiniest new technology or idea.  

tl;dr: I don't think so, but some would call me naive.

## Footnotes
[1] https://finance.yahoo.com/news/typical-unicorn-founder-started-business-120026888.html

[2] I say "monopoly" in the same way [Peter Thiel says monopoly](https://www.youtube.com/watch?v=3Fx5Q8xGU8k&t=5s)


===== ./there-is-always-someone-better-than-you.md =====
+++
title = "There is Always Someone Better Than You (and What You Can Do About It)"
date = "2024-02-01"
+++

### The moment my dreams were shattered (a childhood story)
In the second grade I played in a chess tournament. I made my way somewhat easily to the semi-finals and I still remember exactly how I lost. I remember the exact moves that caused me to fall into a relatively common opening trap (called the Noah's Ark Trap) and how I lost. After the opening, I got destroyed in the middle game and I got smothered mated. Basically he completely embarrassed me. 

The worst part? He was in kindergarten. I cried for a long time. I won some participation thing that my parents tried to cheer me up with, but I was sad for weeks. It wasn't till much later I realized why I was sad, it was because, in that moment, I realized **I would never become a grandmaster**. 

The one thing I was "good" at someone would always be better. 

### My aversion to olympiads and competitions
If it's not clear by now, I am extremely competitive, but I hate competitions. I hate solving problems I can't choose and being subjugated and restricted by a framework imposed upon me. 

I think the reason that IMO winners don't meet their unreasonably lofty expectations for math research, is because they **have not been told to be creative.**

My skills are only partly technical, most of them come from creativity and strong asymmetric convictions.

### Why creativity matters
> "I hate Competition"
> - Peter Thiel

[Link the talk where he said this (highly recommend!)](https://youtu.be/3Fx5Q8xGU8k?si=BweNWg2vDfdA_OvQ)

Steve Jobs was not the best computer or phone maker. He was actually pretty non technical. All he had was a strong conviction about a problem he saw that nobody else did. 

What Thiel is referring to is how there are fundamentally two types of companies: hyper-competitive companies competing for tiny slivers of market share, and disruptors who apply asymmetric thinking to become a monopoly.

Sure, you have to be analytically minded (why do you think technical founders are more successful?), but, you only have to be technical enough to understand the problem. It takes creativity to cast it in a way no one else has and to solve it.

These Olympiads condition kids into thinking that all problems are well defined ones that require patterned solutions. It's a rats race. 

**I don't like only solving problems, I like finding/creating them to.**

### Everyone is creative
Unequivocally, you are the best at being you. You have unique expriences. Apply those to eachother and use your background as a super power to unlock creativty. What can only someone with your specific background create?

### tl;dr
people smarter than you have a shortage of motivation and shortage of problems to work on. 

give them both and pretend to know what you're doing. 

===== ./uncreative.md =====
+++
title = "On Uncreative People" 
date = "2024-08-07" 
+++
There is no shortage of deeply intelligent but characteristically uncreative and cynical people. This is wildly unfortunate. This piece is about one piece of advice I would give this archetype of people.

## the sum of the total is not the total of the sum

Think about cooking. The product has much more "value" than the constituent ingredients alone. 

A painting is much more than just paint and a canvas. 

A song is much more than some 808s and a nice melody. 

This process of turning [profane](https://en.wikipedia.org/wiki/Mircea_Eliade#Sacred_and_profane) things into something sacred (though not strictly divine) is the creative process.[1]

An artist is someone who achieves brilliance by operating on and arranging seemingly common discrete pieces into a continuous, highly compelling piece of art.  

## what does this mean

The people I am writing this article about are the type of people to analytical ascribe value to discrete pieces, sort them by value, and assemble the highest value pieces into their end product. This won't scale superlinearly. 

To them, it's like a test. Each problem is worth x points and you just have to string together enough problems to pass. In this case the sum of the total equals the total of the sum.    

They thrive when given clear goals and metrics. The perfect employee. These type of people are likely to fall into a local maximum. 

Every big or creative idea is attacked, reason is shoehorned to reduce said idea to its "objective" merits. Risks are overly-calculated and conviction is generally low. 

Their intelligence is used to *reduce* to *analyze* to, for the lack of a better word, *antagonize*. Not create. Not to have wild ideas or shift away thinking from the mainstream.  

They live predictably and try to section themselves off from the highly chaotic world we are subject to. In order to get highly "lucky" or escape probabilistic mediocrity you need to be creative. 

Though, there are people who are characteristically unintelligent and uncreative/cynical, these people are liable to be contrarian and wrong. I am not talking about them in this article. 

[1] This parallel between profane/sacred and what divinity really even means in the [hyperreal](https://en.wikipedia.org/wiki/Hyperreality) age is highly interesting. Eliade mostly wrote about prehistoric humans, but the parallels between prehistoric and (for the lack of a better term) "post-modern" humans is worth exploring. 

Consider this [excerpt](https://www.goodreads.com/quotes/541239-we-drove-22-miles-into-the-country-around-farmington-there) for an example of a "sacred" (from the text: "religious experience") and hyperreal (from the text: "taking pictures of pictures") object in modernity. 

===== ./yc.md =====
+++
title = "YC's Scaling Problem" 
date = "2024-10-01" 
+++

## disclaimer
I am not affiliated with YC or have any personal insight into the inner workings of YC. It is hard to track YC's success (with precision) as those metrics are largely not public, and this article is mainly a speculative guess based on limited history. 

## the thesis
Generally, the YC thesis makes a lot of sense. Remove the barrier to entry and networking required to get VC funding and fund smart (curious) people. The requirement for this to work is that YC must be run by smart people with experience. Until now, smart (curious) people have attracted smart (curious) people who in turn attract more smart (curious) people, leading to a sort of snowball effect. Interestingly, this leads to a market where YC is heavily dominant over even the second-best pre-seed specialists, as all the smart people go to YC. 

## the problem
The issue is that the first premise ("Until now, smart (curious) people have attracted smart people...leading to a sort of snowball effect.") is at least weaker now, specifically the "curious" part. 

When YC was less prestigious/known/successful, the only people applying were genuine hackers--people who are "curious". People who like to build. You really just had to carefully filter for people who are smart. The fakers/non-hackers were relatively easy to filter out. YC was generally unattractive/unknown to normal smart people. 

I think YC has recently seen a large increase in smart people who apply who really have no interest in hacking but who are really interested in the idea of a startup and the YC prestige and ethos. Everyone wants to be Zuckerberg; no one has built CourseMatch (or the countless other projects before Facebook). It's not entirely YC's fault that "doing startups" has been excessively mythologized and enveloped in an odd (for the lack of a better term) hyperreality[1][2]; they simply happen to be the best at startups. A good microcosm of this hyperreality is whatever is happening on Twitter. Although, they have been doing a lot of marketing on YouTube and other such platforms. As YC scales, this problem only becomes worse, as more of the wrong type of people will apply.

The reason is, smart, non-hackers' heuristics have landed on doing "startups" and applying to YC as their next step. It's the same drive as wanting to go to a good college; the main driver is prestige. This type of person tends to be *really really* good at dressing up to be something in a way that seems appealing. They excel with checkboxes that lead to rewards, like tests. In fact, Gen Z has been trained to do this via the insidious college application cycle.[3] Since most of the YC philosophy/what they look for is public (via their marketing), those people now have their (albeit, somewhat non-useful) checkboxes. Of course, you can't fake users loving your product, but this makes it harder to judge pre-product companies and identify promising teams. Generally, you can tell if you have fallen to this if you start with "getting into YC" as the **primary** goal of your startup.[4] 

This problem is worse with young people, as we have been practically raised to put on an insane song and dance for getting admitted to college. Also, young people have not had time to develop the pedigree of past projects to accurately judge whether they can "build" or not. There is an opportunity cost in waiting for young people to develop and build stuff, so it is in YC's best interest to try and get in early, which is getting increasingly harder. 

YC was built by hackers for hackers, and only works if they keep attracting hackers.   

[1] I really don't understand this. In an effort to dispel the aura around startups, I will provide a few observations. Most of the rich people I know got rich doing really boring things, live quiet lives, and are happier. Most startups fail, their founders wasting a lot of focused, hard work. Most successful startup founders are considered negatively in the general social consciousness. Most venture-backed startups have founders owning less than 30% of the business at exit, and VCs (usually) get paid first in a liquidation event. Slaving for a decade, not being able to beat your last oversubscribed round on exit, and VCs getting tens of millions while you get nothing is not uncommon. If you are smart, there are so many better "+EV" paths to make money. Don't get caught up in the startup romanticism and distance yourself from "being cracked" or prestige. 

[2] Hyperreality is an often misunderstood concept. I am using it wrongly (colloquially) here and not in a baudrillardian way simply because I can't think of a better word. 

[3] I swear, I saw a tweet from maybe Garry Tan or someone else who said something along the lines of "we are trying to filter out people who treat applying to YC like applying to college". I can't find it now; if someone does, please email me and I will add it. 

[4] There are, admittedly, some exceptions. A very small minority of people follow this path: I want to get rich -> I will create a startup, and actually succeed. I think the percentage of these people who apply to YC is rapidly declining and being replaced by the people described in this essay. This is also different (probably even better) from being prestige-obsessed.

===== ./go.md =====
+++ 
title = "Go is Productive" 
date = "2024-10-30" 
+++

## backstory (skip if only interested in technical explanation)
Yesterday I had a CS midterm from 8-10pm. I was somewhat stressed as I had basically gambled by not properly studying for it. However I finished early, and I was feeling pretty good. Even though it was a Monday night I felt it was cause for celebration. 

So all of us CS kids gathered (and one engineering guy with a chem midterm at 9am the next day...) and broke bread, comforting those who lost their battle with Java/concurrency and decompressing until, perhaps, too late. I was super tired, I had a long day and once everyone dispersed back to their dorms I sat on my couch cracked open my laptop ordered some taco bell and started working on my pet project. 

I awoke a few hours later, laptop sitting on the table next to me and half a crunchwrap that wasn't exactly warm. I ate the rest of the crunchwrap, started some semblance of my morning routine with electrolytes and my vitamin d, and took a look at the project. To my surprise, I found significant progress. It wasn't even like boilerplate, I had finished some complex code. There was properly implemented concurrency, and I basically solved a structural problem where the prompts I was using were half hardcoded, half programmatically generated in Go, and half awk parsed madness baked into the Makefile. I unified them to be in the same place, with the same extensible text format, and all easily accessible across the project in Go. 

The reason for this was I basically knew how to implement it, I had worked that out on my walk back from my midterm, but I just needed to write code. And it turns out, even at my worst, most tired, Go makes it so I can. 

I was so amazed I simply continued coding when I woke up and started replacing all occurrences of hard coding or other weirdness with this system. I tested it, and it worked. 

## technical explanation 
Go is dead simple. It is imperative, has manual error handling, and very few non-obvious anti-patterns. There is no clean or "expressive" way to write highly abstracted code, you just take things and do stuff to them using the same basic, imperative, tools that have been there since C. I rarely have moments where I have to stare at the screen for a long time just to figure out what abstracted clusterfuck has been presented to my eyes. This happened back in my Java days with these insane class hierarchies, happens somewhat frequently in FP (especially point-free APL style languages, which I have a sort of infatuation with), with macros in C, everytime I see C++ templates, and in Rust with 1. Macros 2. FP 3. Basically a whole other language for async code. I am not saying any of those are bad languages, they certainly are fun and have beautiful, satisfying use cases (and I would be lying if I said I didn't miss some features), but Go scratches a different itch. 

For example, I wrote multiple API wrappers (clients, SDKs, whatever) for LLMs, like anthropic, openai, ollama, etc. I wanted to operate on the prompts/messages and have it be generic for all of them, despite the differing formats. Immediately I went to using a monad. Then I was thinking, let me make a generic LLMSdk that is somehow extended by the individual SDKs and the Message type will belong to the generic LLMSdk, And perhaps there is a generic way to do it in Go, but I like having the option to idiomatically not prematurely optimize. What I actually did was just write a bunch of util functions that translated the message types to each other. Is this the most scalable, abstract solution? no. Did it work and not increase the complexity of the project at all? yes. 

I wrote the backstory because I feel like I can rip through 8+ hours straight of simple Go CRUD (more in the literal sense) using very little of my brain on how to express things "nicely/beautifully" using the language, but instead focussing on what I actually want the application to do. I almost never think "well, is this the most idiomatic way of doing this?". Sometimes I just want to put the stuff into some structs, do some operations, throw it in the database and be done. You don't have to be "smart" to write Go, and frankly, being "smart" is overrated.  

I love coding and solving/doing hard problems, but when I am working on a product, I want to use my brain to solve product issues, not implementation issues. I want the inertia I need to do tasks to be low, so I don't have to eat 1 million complicated, highly technical annoying frogs everyday. Coding in Go literally feels like folding laundry, basically doing the same thing repetitively with little mental overhead, things happening automatically, and I love it. This autonomical experience is basically hitting flow state for me. I don't have to think about how do I do X, I just have to think I want my product to X and then start spewing code into my IDE. 

## limitations
Like all matters, there are some caveats. If your X (thing you want to implement in your product) is not something well suited to basic business logic CRUD (ok maybe a little more advanced than that) or is a specialized CS topic, Go probably is not for you. Like if you can take a masters level class in it, you probably don't want to use Go. Some examples would be compilers/parsers, ML/AI, operating systems, etc. Though even then, concurrency and network IO are still pretty easy to do in Go. It also has some good built in crypto libraries and standard algorithms.   

You can also make the argument this experience is just because I am more familiar with Go than those other languages, and the ease of it compared to the other languages is just a skill issue. I would say I feel as if there is an upper limit to skill, and most languages have a skill ceiling so high they there exists terse, syntactically correct statements that would take experts a long time to decipher semantically. I consider myself fairly proficient in Java, but some abstractions just take time to piece together how they work.  The implementation has little to say with **what** it is doing (its goal) and a lot to say about the **manner** in which it will go about doing it. So it's hard to figure what is actually going on, but easy to realize how "clean" and "extensible" and "DRY" it is. If you think about it, this is analogous to what point-free syntax is, you describe functions/transformations over your input, but never explicitly mention doing stuff to it.  

===== ./entropy.md =====
+++ 
title = "In Defense of Colloquial Entropy" 
date = "2024-11-11" 
+++

## purpose
Physicists long balked at colloquial usages of "entropy" (especially by philosophers). I believe some usages of entropy as resistance to systematic understanding has precise mathematical grounding in information theory. I am tired of arguing with my physics major friends over this. 

## definitions
Shannon entropy H(X) for discrete random variable X:
```
H(X) = -∑ p(x) log₂(p(x))
```
measures average bits needed to encode X's outcomes.

For any compression scheme C mapping sequences of X to bit strings:
```
L(C) ≥ H(X)
```
(Shannon's source coding theorem)

In statistical learning with probability distribution P and hypothesis space H:
```
Error = Bias² + Variance + Irreducible Error
```
(Bias variance trade off)
## phenomenological mapping
Consider raw experiential phenomena:
1. Emotional states (continuous, high-dimensional)
2. Complex situations (multiple interacting agents/factors)
3. Novel problems (undefined solution spaces)
4. Transient physical experiences (direct sensory input)

These can be mapped to information-theoretic framework:
- Each phenomenon generates sequence of observations
- Observations drawn from probability distribution P
- Mental models attempt compression into predictable patterns
- Compression complexity bounded by Shannon entropy

## claim
Consider this tweet I made:
> you interpret via self-constructed systems. you can't out-construct entropy (overfitting) => interpretation/discursive analysis is often overused. sit with anxiety inducing things before subjugating it with your pretty little intellectual frameworks.

Let's prove this has mathematical meaning:

1. Map experiential phenomena to data points from distribution P
2. Mental models are compression schemes C with complexity L(C)
3. For experience with entropy H:
   - By Shannon's theorem: L(C) ≥ H
   - Equality holds only for lossless compression
4. By bias-variance tradeoff, cannot simultaneously:
   - Minimize model complexity L(C)
   - Capture all patterns in P
   - Generalize to new experiences

Therefore: For high-entropy phenomena, any attempt to "out-construct" (compress below H bits) must either:
- Lose essential information (underfitting)
- Create brittle, overfitted models
- Accept fundamental incompressibility

## analogous failure modes (AI)
Just as in machine learning (literally, *artificial* intellect) with high-entropy data, we encounter:

1. In compression:
   - Information loss through simplification
   - Over-specified models that don't generalize
   - Incompressible sequences requiring full description

2. In experiential interpretation:
   - Oversimplified frameworks missing crucial nuance
   - Complex frameworks that fail on new experiences
   - Phenomena resisting systematic understanding

## (broad/loose/quick) applications
Consider interpreting complex emotion:
1. Low-complexity: "I'm just sad" (1-bit classification, high bias)
2. High-complexity: "Unique combination of childhood memory #247, recent event #892, weather conditions..." (high variance)
3. Optimal: Maintain full complexity when compression would lose essence

Similarly for novel problems:
1. Low-complexity: Force into known solution patterns
2. High-complexity: Enumerate every possible factor
3. Optimal: Hold uncertainty while patterns emerge

These applications are analogies as even putting our experience in words is subjugating them to some form or system of human understanding/interpretation.
## implications
1. For high-entropy experiences (high Kolmogorov complexity relative to description language), compressed representations must lose information
2. Cannot build frameworks that are simultaneously:
   - Simple enough to be useful
   - Complex enough to capture patterns
   - Generalizable to new experiences
3. Raw phenomena often contain more information than any compressed representation can maintain

## conclusion
Not all phenomena permit compression. Just as there exist mathematically incompressible sequences, there exist experiences whose true information content exceeds our capacity for systematic representation. The limit isn't practical but mathematical.

Sometimes the most accurate model is the experience itself, held in its full entropy without reduction to simpler frameworks. This isn't an argument against systematic understanding, but a precise delineation of its boundaries. This has been long known by vedic/buddhist thinkers.  

My physics UG friends objecting to colloquial "entropy" ironically demonstrate my thesis--attempting to compress a mathematically valid analogy into overly rigid framework.

## references 
- wikipedia 
- the inner depths of my mind
- [**this book which is the best resource on entropy I have ever read**](https://arxiv.org/pdf/2409.09232)


===== ./llm_learning.md =====
+++
title = "How LLMs Actually Make Better Programmers (and further pedagogical quibbles)" 
date = "2024-11-07" 
+++

## premise 
After I saw that Github Copilot was able to do my DSA coursework with little guidance (though, I never did use it to cheat) I for sure thought that LLMs would ruin kids ability to learn how to code and that it would handicap their learning of not only coding but the meta-cognitive skills around learning coding (ie, "learning how to learn"). 

In the past few weeks my view has switched entirely. I write this article to help people understand the otherside. 

## argument 
I have been extensively using LLMs to code maximally, as [detailed in this article](https://rohan.ga/blog/llm_workflow/), but only hit my peak productivity a few weeks ago when I fixed my prompts and tooling (and o1 came out). This period feels very reminiscent of when I was a "script kiddie" (or skids, as the young people call them) back in the day.[1] I would copy code from random places and basically pray that my hodgepodge would compile and work, if it didn't I'd ask on stack overflow or IRC or discord (often amassing mass downvotes and bad rep). This period was a necessary and integral part of my (and I am sure, many others) success and learning. 

The only difference is that now "script kiddies" can accomplish much more using their traditional, age old, heuristics, and hit harder problems faster.[2] This is good for two reasons. First, it raises the floor of coding, more people can create more better things. Second, it accelerates the traditional learning process by forcing people to solve harder problems. The issue increasingly becomes one of curiosity and motivation to solve these harder problems. Curiosity is one of the strongest tenets of hackers (and people in generally) and will filter out people who probably should not be programmers in the first place, while decreasing the barrier to entry for even more people who do not fit the traditional mold.[3] Even back in the day, many did not graduate from "script kiddie".  

 What I mean by this specifically is that people generally associate programming with "smart" or "nerdy" people and are less likely to try it themselves, especially given system disadvantage. With LLMs more of the people who lack access or systemic advantages or who are societally prodded away from coding can more easily learn. I would have to guess if I was not born in the bay area to two technically inclined parents as an Indian male that I would probably not be a coder (which is an insight I tend to go back and forth on, but I generally consider to be true).[4]

At the end of the day, when you encounter an error or limitation in an LLM, you have to be able to fix it.[5]  The only way to fix it is to learn something. The only difference is you learn more things faster by tackling harder problems and if you don't understand something you have access to a 24/7 intelligent entity. The democratization of intelligence is a wonderful thing, analogous to the internet and stuff like stack overflow, except now the latency has reduced. 

## calculator (internet?) analogy 
This analogy has been beaten to death, so feel free to skip it. 

People said the calculator would ruin learning math. It didn't, and, in the same way we have calculator and non-calculator tests[6], we are going to have AI and non AI tests. I still find the balance important and think that to "formally" learn things on a concrete level, you probably need to forgo LLMs.

I think the internet is also partly analogous just due to the mission of democratizing information. Now, we are democratizing intelligence. I don't really know how it was with the internet, but I am sure some some white beard GNU wizard who was coding 30+ years before my birth made the argument that kids who did not start on mainframes, punchcards, and manuals/books have it easy and they don't know how to be a "real programmer". (Absolutely no hate to white beard GNU wizards, they are wizards for a reason). 

[1] The key difference being that if the code doesn't integrate properly I generally know immediately why and can start integrating or redoing my prompts, making it much more productive.  

[2] They also save the time of experienced users by having commoditized access to intelligence, so their questions don't clog up discords, forums, IRCs, etc.   

[3] A similar problem exists in math, where people generally think back to their Calc 1 class and say "yeah I sucked at math" when either 1. their teacher was bad 2. they didn't even try because people have been telling them they are bad for their whole life or 3. they didn't even try because they think they hated math. In order to reach the good math (proof based math, which basically requires a whole different host of skills), the people who are good and would like the good math are effectively screened out. [Math is literally the most switched out of majors because of this.](https://nces.ed.gov/pubs2018/2018434/index.asp#:~:text=About%20half%20(52%20percent)%20of,STEM%2C%20except%20the%20natural%20sciences.) (probably).

[4] Personally, I did not know I could write or analyze text at the average level until high school, I just always thought I was a STEM guy who was bad at English before that...

[5] If you *don't* run into issues like this, or you can *always* fix them by prompting, I think you found AGI...

[6] I also think non-calculator tests are a symptom of a bigger problem (at least in my experience). I want to preface this by saying the way I learn things is atypical. If I can't use a calculator because it impedes my understanding of the underlying math, why can you not just teach us something that presupposes understanding of the underlying math? 

For example, instead of having me manually multiply matrices without a calculator to prove I understand matrix operations, just teach me linear transformations and ask me to solve problems that require actual understanding--like explaining why matrix multiplication isn't commutative using geometric intuition, or determining if a transformation is invertible by analyzing its properties. Someone who truly understands matrices isn't going to be hindered by using a calculator for the arithmetic. Maybe it would be helpful to know how to manually multiply matrices, but testing that has to be one of the worst ways to test for understanding. I frequently partake in rebellion against this in my CS classes by spending time memorizing every algorithm I would need to know and spitting out a super terse hard to grade version on the test. Though I think my anger has befallen the poor TA instead of the relevant systemic forces... 

I also often used tricks of playing with the calculator and noting down the invariants when solving those types of problems, which is basically learning on the fly while taking the test. I can also do the same thing manually if I remember a rule but not the proof. This is super fun (because you are actually doing math the way god intended) and minimized the time spent studying (instead relegating to "uhh, hopefully I can figure it out on the test").  

I also think this ties back to my math discussion in footnote 3 and how current math pedagogy is screening and filtering the wrong people. 

===== ./moats.md =====
+++ 
title = "MOATs Aren't Useful" 
date = "2024-11-05" 
+++

## clarification
What I mean is that, conceptually, "MOATs" are not a useful abstraction. There is a saying that ["all models are wrong, but some are useful"](https://en.wikipedia.org/wiki/All_models_are_wrong) and I am saying that, as a model, MOATs are not useful. I think that asking a founder "what their MOAT is" will not offer useful information about their project.

I am also specifically referencing tech startups.  

## what is a MOAT
From what I gather, a MOAT is defined as "a company's (unique) ability to maintain a competitive advantage that protects its long-term market share and profits."[1]

## why it's not useful 
A company's ability to maintain a competitive advantage is solely dependent on people wanting to use their product. If more people want to use your product than your competitors product, you've won. In other words a MOAT is just a good product, and as to "protecting its long-term market share and profits" that is a skill issue, a bet on the founders. Even if someone copies your product, they don't have your vision that spawned the product in the first place. Big companies also don't allocate resources to taking the same product risks as startup founders--they stick to what makes them money.  [Further, the whole prospect of "competing" with other companies is an ineffective framing.](https://www.youtube.com/watch?app=desktop&v=3Fx5Q8xGU8k) There is rarely one clear "MOAT" that founders can point to, just their vision, and how many people love their product. Asking "what's your MOAT" may be the worst way to get founders to communicate this, because they are more likely to make something up that they think you want to here, or worse, they might actually believe whatever they told you.

## why people think they need MOATs 
The traditional thinking goes: without some special advantage that others can't replicate, competitors will copy your success and eat your lunch. This makes sense if you're selling commodity widgets. It makes zero sense in technology.

Look at the major tech successes:
-   Facebook wasn't the first social network
-   Google wasn't the first search engine
-   Apple wasn't the first computer company
-   Amazon wasn't the first online retailer

None of these companies succeeded because of some mystical MOAT. They succeeded because they built something people wanted and kept building. The "MOAT" emerged as a side effect of execution, not as some pre-planned strategic advantage.

## traditional explanations of "MOATs" are bad
A startups success is not a static victory protected by a MOAT--it's an iterative process of understanding users and shipping improvements. When founders try to articulate their MOAT, they usually default to one of these weak explanations:

1.  "Network effects" (which only matter if you have a product people actually want to use)
2.  "Proprietary technology" (which becomes commoditized surprisingly quickly)
3.  "First mover advantage" (meaningless - see the examples above)

If you optimize for these things you will lose sight of actually just making stuff people like to use, which is all that matters. These are symptoms, side effects. 

### P.S. If you can learn it in business school, it's probably wrong.  

[1] Summarized off [this article](https://www.keshav.wtf/post/moat) which is the top google result. No judgement levied against the article or its writer, I just wanted to synthesize a fair definition to base my argument off of. 



===== ./chess_llm.md =====
+++ 
title = "o1-preview is Pretty Good at Chess (a New Benchmark?)" 
date = "2024-11-14" 
+++
# overview
I figured the reasoning capabilities would lend themselves to chess, and they do. Right now I am limited by the 50 o1-preview requests I have to do research in this area so keep this mind when I share.

# testing vs stockfish
I was playing o1-preview against stockfish (via lichess), and it held up for the most part, but stockfish was winning when I ran out of tokens. It also played a lot of theory, we transposed from a psuedo-catalan to a neo-grunfield. I was the one playing white for the opening and just played the theory I knew. (find game record at the end)

From what I can tell, it only generally uses a depth of 2-3 and its breadth is much narrower than other chess engines. This is characteristic of a human. Once inference time compute is scaled, I think that the breadth and depth will increase to a point where it will at least match Stockfish and other top engines.

# technical challenges & solutions
Around move 25 it started spitting nonsense moves, so I looked at what the reasoning traces said and I saw it try and process and step through the PGN or recreate the FEN in a different internal representation. So I asked it what format it would like the board state to be communicated in and to make a python script to generate them.

After using the script it made (with some modifications), it replied with valid moves. Put your FEN into the [script here](https://gist.github.com/Ocean-Moist/0064caf583bc4ccf4151fd1a50601836) and paste the output into o1-preview to get your next move. Perhaps if I used this from the beginning the moves would have been of a higher quality. 

# cognitive similarities to humans
What's interesting is that the same things that would make it readable to a human make it readable to the machine. Originally it seems it was first interpreting the PGN/FEN into a format it understood and then doing reasoning from there. This is the same thing humans do. They have a internal representation for a chess board that they can visualize transformations of it, most chess pros are used to pgn so they communicate their thoughts via that. This is what allows chess pros to play games without a board, or blindfolded.

I wonder if similarly, if you finetune a model to communicate via PGN (chess puzzles, calculating lines, playing games), instead of relying on inference time interpretations, you will notice improved performance. This would allow it to build an internal representation of the chess board. If you notice chess pros playing, they often look up and to the left/right to visualize different board permutations and continuations when calculating.

# potential as a benchmark
I would love to automate this and make it a real benchmark for reasoning models, as this seems like a new capability of them vs. previous LLMs. I estimate o1-preview is around ~1600-1800 chess.com, though take this with a massive grain of salt (n=0.8, I am around 1900 chess.com). It would be hard as each game would cost you a bunch of money. I found that o1-mini has a much harder time providing coherent moves.

# game record
PGN: 1. Nf3 d5 2. d4 Nf6 3. g3 g6 4. Bg2 Bg7 5. O-O O-O 6. c4 c6 7. Nc3 e6 8. cxd5 exd5 9. Ne5 Nbd7 10. Nxd7 Qxd7 11. Re1 Re8 12. Bf4 Nh5 13. Be3 f5 14. Qd2 Qf7 15. Bh6 Bxh6 16. Qxh6 Nf6 17. Rac1 Ng4 18. Qd2 f4 19. Qxf4 Qxf4 20. gxf4 Nf6 21. Na4 Nh5 22. e3 Nf6 23. Nc5 b6 24. Nd3 Ba6 25. Rxc6 Bxd3 26. Rxf6 Rad8 27. Rc1 Bc4 28. b3 Kg7 29. Rc6 Rc8 30. Rxc8 Rxc8 31. bxc4 b5 32. c5

===== ./eee1.md =====
+++
title = "Experience, Entropy and the Ephemeral - Part I"
date = "2024-11-29"
+++

## I. The Ground of Experience

We find ourselves always already in experience. This is not an ontological claim but a recognition of our starting point. Any attempt to ground experience in something more fundamental—consciousness, matter, information—already imposes construction on what is given. Even to speak of "what is given" suggests a giver and receiver, a division we cannot justifiably make at this level. In a way, the rawness of this "pure" experience is ineffable much in the same way God is ineffable in Platonic models.

Within this ground of experience, resistance manifests. Not as something external pushing back against our will or understanding, but as an intrinsic feature of experience itself—emergent. Every attempt to fully systematize, comprehend, or capture experience encounters this resistance. It appears as the incompressibility of the present moment, the inability to fully articulate a sensation, the gap between map and territory.

This resistance bears the character of entropy—a tendency toward dissolution, an arrow of time, an inevitable loss of distinction and order. But we must be careful here. To call it entropy is already to construct, to impose a scientific framework on that which precedes science. Yet this construction proves useful, illuminating aspects of resistance that resonate across domains of experience.

## II. Temporal Subjugation and the Production of Anxiety

Our most fundamental subjugation is to time. Unlike space, which we can traverse freely in any direction, time holds us in its irreversible flow. This temporal subjugation is not merely a limitation but a constitutive feature of experience that generates both anxiety and productivity. Each moment slips away as it arrives, creating a perpetual loss that we can neither prevent nor fully accept.

This temporal character of experience creates an existential tension, perhaps *the* existential tension. We strive for permanence while being composed of impermanence. We seek to create lasting meaning while being swept along in time's current. This tension manifests as a fundamental anxiety—not a psychological state that some experience and others don't, but a basic feature of temporal existence that underpins all experience.

This anxiety proves productive. The very impossibility of escaping time's flow drives us to create, to build, to systematize. We construct languages to capture fleeting thoughts, build monuments to preserve passing moments, develop sciences to predict future states, create art to crystallize ephemeral beauty. All human production can be understood as arising from this temporal anxiety—attempts to achieve some permanence in the face of inevitable dissolution.

## III. The Nature of Resistance

The resistance we encounter in experience takes many forms, but always bears certain characteristics. It manifests as the excess that escapes our categories, the remainder that resists our equations, the ineffable quality that slips through our words. This resistance isn't arbitrary but patterned—some constructions fail immediately, others work remarkably well, most occupy a middle ground of partial success.

This patterned resistance suggests structure without implying substance. Like the wind made visible only through the movement of leaves, resistance appears through the failure of our attempts at complete capture. This is not a negative theology, defining reality only by what it is not. Rather, it is a recognition that resistance itself is productive—generating both the possibility and limitations of our constructions.

The entropy-like character of this resistance connects to our temporal subjugation. Time's arrow creates an inevitable gradient toward dissolution, yet this very gradient enables the emergence of structure. Like a river whose flow both erodes and creates, temporal entropy both destroys and enables construction. This dual character—destructive and productive—proves essential to understanding the nature of emergence and the possibility of meaning.

**[part 2](https://rohan.ga/blog/eee2/)**

===== ./eee2.md =====
+++
title = "Experience, Entropy and the Ephemeral - Part II"
date = "2024-11-30"
+++

## IV. Construction and the Architecture of Understanding

Our constructions arise not from free choice but from necessity—driven by temporal anxiety and shaped by patterns of resistance. Language, mathematics, science, art, philosophy—these emerge as attempts to build stability in the face of dissolution. Yet they are not mere defenses against time's flow. Each construction opens new possibilities even as it encounters its own limitations. Further, free choice is itself a construction, perhaps one imposed by the evolutionary algorithm, teasing out a connection between the supposed "freedom" of human experience and its productive outputs.

Language serves as the primordial construction. Before formal systems, before scientific theories, we find ourselves already within language. It shapes thought even as thought shapes it, creating the categories through which we understand experience. Yet language itself exhibits the dual character of construction—enabling comprehension while inevitably falling short of full capture. The ineffable always remains, not as mystical transcendence but as intrinsic resistance to complete systematization.

Mathematics appears as the purest construction, seemingly freed from temporal contingency. Its truths strike us as eternal, its patterns as transcendent. Yet mathematics too emerges from temporal existence—from the need to predict, control, and understand patterns in experience. Its power lies not in escaping time but in constructing frameworks that navigate temporal flow with remarkable precision. The seeming timelessness of mathematical truth reflects not transcendence but the discovery of particularly stable patterns in resistance. Even then, its resistance is obvious, demarcated by supposed axioms and the existence of undecidable statements (in the proof-theoretic sense) in any given system.

Science extends this constructive project (not without the losses of mathematics "purity"), building systems to predict and manipulate experience. Its success comes not from discovering ultimate reality but from developing increasingly sophisticated ways to navigate resistance. Scientific theories are not progressively more accurate pictures of reality but more effective tools for managing our encounter with resistance. Their power lies in their utility, not their correspondence to some underlying truth.

## V. Power and the Production of Knowledge

Power operates not as external force but as intrinsic feature of construction. Every system of understanding, every framework of meaning, every institution of knowledge embodies and enables power relations. This is not a corruption of pure knowledge by power but recognition of their inseparability. Power shapes what questions we ask, what answers we accept, what forms of knowledge we validate.

The temporal character of existence makes this power operation inevitable. Our need to construct meaning in the face of dissolution creates hierarchies of understanding, institutions of knowledge, systems of validation. These structures don't simply constrain—they produce. They generate new forms of knowledge, new possibilities for understanding, new modes of being.

This productive aspect of power reveals itself in the very anxiety that drives construction. The temporal tension that forces us to build systems of understanding also creates the conditions for control. Those who manage meaning-making, who direct productive anxiety, who institutionalize certain constructions while delegitimizing others—they exercise power through the very structures that make understanding possible.

Yet power, like all constructions, encounters resistance. No system of control achieves total dominance. The same resistance that necessitates construction also ensures its incompleteness. This creates space for alternative constructions, different systems of meaning, new forms of understanding. Power operates not through absolute control but through ongoing management of construction and resistance.

## VI. The Space of Emergence

Between rigid construction and pure resistance, a space of emergence appears. This is not a physical space but a zone of possibility where new patterns, meanings, and structures can arise. Like the turbulent flow between laminar and chaotic fluid dynamics, this space enables complexity through its very instability.

Consciousness itself might be understood as an emergent phenomenon of this middle space. Neither purely constructed nor simply given, consciousness arises from the dynamic interaction between order and entropy. Its characteristics of unity and continuity emerge not from central control but from dynamic self-organization in this space of medium entropy.

Beauty too belongs to this emergent realm. Not as subjective preference nor objective property, but as pattern that emerges from the interplay of construction and resistance. Beauty appears in the tension between order and chaos, in the moment when construction achieves dynamic equilibrium with resistance. This explains both its universality and its ineffability—it emerges from common patterns of resistance yet resists final capture in any system.

The very possibility of innovation depends on this space of emergence. Too much order stifles creativity; too much chaos prevents coherence. Innovation requires both constraint and freedom, both structure and possibility. It arises from the productive tension between our constructions and their inevitable incompleteness.

**[part 3](https://rohan.ga/blog/eee3/)**

===== ./eee3.md =====
+++
title = "Experience, Entropy and the Ephemeral - Part III"
date = "2024-12-01"
+++

## VII. Navigation in Temporal Existence

To navigate is not to escape. We cannot step outside temporal existence, cannot free ourselves from the anxiety of dissolution, cannot build perfect systems that fully capture experience. Navigation requires sophisticated engagement with these very limitations. It means developing dynamic equilibrium with resistance rather than attempting to overcome it.

This navigation operates simultaneously across multiple registers. In thought, it manifests as the ability to use constructions while recognizing their limitations—to think systematically without being captured by systems. In practice, it appears as the capacity to build effectively while accepting impermanence—to create without demanding permanence. In relation to power, it emerges as the skill to work within and against existing structures—to use institutions while maintaining awareness of their constructedness.

The temporal character of existence makes this navigation inherently unstable. Any equilibrium achieved is temporary, any balance struck is dynamic. Yet this very instability enables creativity. The constant need to readjust, to recalibrate, to respond to changing conditions creates opportunities for innovation. Like a skilled surfer riding a wave, effective navigation requires constant micro-adjustments in response to shifting conditions.

This mode of navigation differs fundamentally from the project of mastery that characterizes much of modern thought. Instead of seeking to control or overcome resistance, it aims to develop sophisticated relationships with it. This requires cultivating sensitivity to patterns of resistance while maintaining flexibility in response. It means learning to read the grain of experience without becoming fixed in any particular reading.

## VIII. Creation and the Production of Meaning

Creation in this context takes on a different character. Rather than ex nihilo production or simple combination of existing elements, creation emerges from sophisticated engagement with patterns of resistance. Like a sculptor revealing form within marble, the creator works with rather than against the grain of material. Yet this material is not physical substance but the patterns of resistance encountered in experience.

This understanding transforms how we approach meaning-making. Meaning emerges not from correspondence to external reality nor from pure social construction, but from successful navigation of resistance patterns. A meaningful framework is one that enables effective navigation while acknowledging its own limitations. This explains both the plurality of meaning systems and their non-arbitrary character—different constructions can navigate successfully while none can claim absolute truth.

The temporal dimension remains crucial here. Creation always occurs under pressure of time, driven by anxiety of dissolution yet enabled by this very pressure. The ephemeral character of existence creates both the need and the possibility for meaning-making. Like a jazz musician whose creativity emerges from the pressure of real-time improvisation, human creation generally draws power from temporal constraint.

Power continues to operate in this creative process, but not merely as constraint. Power relations shape what creations become possible, what meanings become legitimate, what innovations get developed. Yet power itself depends on successful navigation of resistance patterns. Systems of power persist not through pure domination but through effective management of the tension between construction and resistance.

## IX. Implications for Knowledge and Truth

This framework suggests a fundamental revision in how we understand knowledge and truth. Knowledge appears not as representation of reality but as successful navigation of resistance patterns. Truth emerges not as correspondence to external facts but as effective engagement with the constraints and possibilities of experience.

This shifts epistemological questions in a crucial way. Instead of asking whether a framework is true or false, we might ask how effectively it enables navigation of experience. Instead of seeking foundations for knowledge, we might explore patterns of resistance that shape what constructions become possible. Instead of trying to eliminate uncertainty, we might develop more sophisticated ways of engaging with it.

Science in this light appears as sophisticated pattern recognition and navigation rather than discovery of ultimate reality. Its power comes not from accessing objective truth but from developing increasingly effective ways to engage with resistance patterns. This explains both its remarkable success and its inevitable limitations. Scientific theories work not because they mirror reality but because they capture important patterns in how experience resists certain constructions while enabling others.

Similarly, mathematics appears not as transcendent truth but as exploration of particularly stable patterns in resistance. Its seeming necessity reflects not access to Platonic forms but discovery of constraints that remain invariant across wide ranges of experience. This explains both mathematics' unreasonable effectiveness and its ultimate incompleteness.

## X. The Future of Construction

Looking forward, this framework suggests new approaches to persistent problems. Consciousness, for instance, might be better understood not as internal state or emergent property of physical systems, but as pattern of navigation in the space between construction and resistance. Free will might be reconceived not as metaphysical capacity but as sophisticated engagement with patterns of possibility.

More practically, this understanding suggests new approaches to technology, social organization, and cultural production. Rather than seeking to eliminate uncertainty or overcome limitations, we might focus on developing more sophisticated ways to navigate them. Rather than trying to build permanent solutions, we might create systems that effectively engage with impermanence.

The future belongs not to those who solve the anxiety of temporal existence, but to those who transform it into creative production. Not to those who build perfect systems, but to those who maintain dynamic equilibrium with resistance. Not to those who claim absolute truth, but to those who enable effective navigation of experience.

In the end, this framework itself must be understood as construction—one more attempt to navigate patterns of resistance in experience. Its value lies not in final truth but in effective enablement of navigation. Like all constructions, it will encounter its own limitations, generate its own resistances, require its own transformations. Yet in acknowledging this, it opens new possibilities for engaging with the fundamental conditions of temporal existence.


===== ./java-go.md =====
+++
title = "Go and Java: Rethinking Type Safety for the Pragmatic Age"
date = "2024-11-26"
+++
## premise
I want to explore where mainstream programming languages are headed, using Java and Go as my primary subjects.

## java is underrated
The reason I say Java is underrated is because it is oddly both easy to learn and its type system is strong enough to make many incorrect states irrepresentable.[1] It has product types and sum types, and makes managing state explicit and easy enough.[2]

Java is very "simple" precisely due to its verbosity. This doesn't mean all Java code is simple. Some people manage to abuse inheritance, etc., to make very complex, hard-to-process Java, but I argue this is due to ideology and that it is easier to do things in a simpler, more correct way. The ideology around Java is the main reason it is generally overlooked. Spring Boot is a perfect example of this, although more reasonable alternatives like Javalin exist. tl;dr: Java is bad if you are midwit.

Take Go, a much simpler language—you lose the safety of the more complex type system but gain ease of use and idiot-proof it against people doing things in a dumb way. Sometimes in Go, my programs' complexity warrants a better, more fool-proof type system. It feels like I am forced to [validate not parse](https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/).

That said, Go's achievements shouldn't be understated. It has largely eliminated whole categories of runtime errors through static typing and careful language design. But in complex systems, we still encounter what I think of as "semantic drift"—programs that are syntactically perfect but semantically suspect. Edge cases multiply; unexpected states proliferate.

## towards the future
What's needed is a language combining:
-   The expressiveness of sum types and product types
-   The clarity of errors-as-values
-   The safety of null absence
-   The simplicity of removing inheritance
-   The practicality of minimal dependencies

The issue is that PL people making languages don't understand the Grug developer or don't have the distribution to pull it off. Hats off to Rob Pike and Ken Thompson for their understanding of the (below?) average dev.

I am basically just describing my vision for Go 2.0, but I think where I differ from many people is that:

**I think that Go is Java's spiritual successor and that Go 2.0 would be (somewhat) similar to Java**

Java's motto was "write once, run anywhere," which is similar to "simple, secure, stable." The dichotomy between "once" and "anywhere" points to stable; "run anywhere" implies security (you must be secure if your code runs anywhere), and perhaps what needs work is the "simple."

A lot of what people would have written in Java is getting written in Go nowadays: grim enterprise backend code abused like a workhorse. Perhaps a stronger type system is needed; we already got generics.

## closing
I don't want anyone thinking I think Java is better than Go—Go got a lot of the easy stuff right. The concurrency is good, the compiled nature (vs JVM/runtime) is good, the libraries and ecosystem are so much better, and the build system and tooling are miles better as well.

PS: If you think I am stupid and don't know anything about languages, this makes my argument stronger, as then I am Go's target audience.

[1] When you take this lens, you can see the argument for exceptions—errors should be _unrecoverable_. The checked vs. unchecked distinction in Java actually undermines this by forcing handling of what should be truly exceptional circumstances. Or rather, there are sometimes where incorrect states must explicitly be represented and managed and are not exceptional. This observation is what spurred errors as values.

[2] My use of "sum/product types" deserves clarification. While Java's enums and classes aren't pure algebraic data types in the formal sense, they provide workable approximations that serve similar purposes in practice. Classes act as product types with methods attached—a pragmatic compromise between mathematical purity and engineering utility. Modern Java features like records (pure product types) and sealed classes (constrained inheritance hierarchies) bring us closer to true ADTs, though gaps remain. Enums paired with sealed interfaces offer a reasonable approximation of sum types, even if they lack the elegant pattern matching of ML-family languages. This imperfect but practical type system allows us to model domain concepts with sufficient rigor while remaining accessible to mainstream developers. The pattern matching could be a bit better as well but it's *good enough*.

